{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Enhanced E-Reader.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "g44o97HERh0H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#This code takes a Project Gutenberg book and uses Wikify to add in links to make an enhanced e-Reader.\n",
        "#NOTE: THIS CODE CAN TAKE AWHILE TO RUN\n",
        "#Made for DSCI 511 in December 2018\n",
        "\n",
        "USER_KEY = 'vvtmxtinhohcxaxqgdrleivlgpqxok'\n",
        "\n",
        "import requests\n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "from IPython.display import IFrame\n",
        "import os\n",
        "import urllib.parse, urllib.request, json\n",
        "from IPython.core.display import HTML\n",
        "import re\n",
        "\n",
        "def Gutentag(ID):\n",
        "    file_path = 'https://www.gutenberg.org/files/' + ID + '/' + ID + '-h/' + ID + '-h.htm'\n",
        "    data = requests.get(file_path).content\n",
        "    with open(ID+'.htm', 'wb') as outfile:\n",
        "        outfile.write(data.text)\n",
        "        \n",
        "def wikify(text, lang=\"en\", threshold=0.8):\n",
        "    # Prepare the URL.\n",
        "    data = urllib.parse.urlencode([\n",
        "        (\"text\", text), (\"lang\", lang),\n",
        "        (\"userKey\", USER_KEY),\n",
        "        (\"pageRankSqThreshold\", \"%g\" % threshold), (\"applyPageRankSqThreshold\", \"true\"),\n",
        "        (\"support\", \"true\"), (\"includeCosines\", \"false\"), (\"ranges\", \"false\")])\n",
        "    url = \"http://www.wikifier.org/annotate-article\"\n",
        "    req = urllib.request.Request(url, data=data.encode(\"utf8\"))#, method=\"POST\")\n",
        "    with urllib.request.urlopen(req, timeout = 60) as f:\n",
        "        response = f.read()\n",
        "        response = json.loads(response.decode(\"utf8\"))\n",
        "    return response\n",
        "\n",
        "def build_doc(wiki):\n",
        "    orig_text = \"\"\n",
        "    for i in range(len(wiki['spaces'])):\n",
        "        orig_text = orig_text + wiki['spaces'][i]\n",
        "        if i <= len(wiki['words'])-1:\n",
        "            orig_text = orig_text + wiki['words'][i]\n",
        "        else:\n",
        "            continue\n",
        "    return orig_text\n",
        "\n",
        "        \n",
        "def get_links(wiki):\n",
        "    links = []\n",
        "    for annot in wiki['annotations']:\n",
        "        for support_dict in annot['support']:\n",
        "            links.append((support_dict['chFrom'], support_dict['chTo'], annot['url']))\n",
        "    return links\n",
        "\n",
        "def build_link(doc, start,end, dest):\n",
        "    link = doc[start:end+1]\n",
        "    format = \"<a href=\"+dest+\">\"+link+\"</a>\"\n",
        "    return format\n",
        "\n",
        "def embed_link(doc, link):\n",
        "    start = 0\n",
        "    n_doc = \"\"\n",
        "    for l in link:\n",
        "        n_doc = (n_doc + doc[start:l[0]]+build_link(doc,l[0],l[1],l[2]))  \n",
        "        start = l[1]+1\n",
        "    nn_doc = n_doc+doc[start:]\n",
        "    return HTML(nn_doc)\n",
        "\n",
        "def embed_links(wiki):\n",
        "    doc = build_doc(wiki)\n",
        "    links = get_links(wiki)\n",
        "    sorted_links = sorted(links, key=lambda x: x[1], reverse=True)\n",
        "    valid_links = []\n",
        "    for i in range(len(sorted_links)-1):\n",
        "        if sorted_links[i+1][1] < sorted_links[i][0]:\n",
        "            valid_links.append(sorted_links[0])\n",
        "    valid_links.append(sorted_links[-1])\n",
        "    link = valid_links[::-1]\n",
        "    return embed_link(doc, link)        \n",
        "\n",
        "\n",
        "def get_out_path(path):\n",
        "    path_list = path.split('/')\n",
        "    out_dir = path_list[:-1]\n",
        "    file_name = path_list[-1]\n",
        "    file = re.split('.htm', file_name)[0]\n",
        "    out_name = file + '.json'\n",
        "    out_dir.append(out_name)\n",
        "    out_path = '/'.join(out_dir)\n",
        "    return out_path\n",
        "\n",
        "def wikify_html(path):\n",
        "    out_path = get_out_path(path)\n",
        "    with open(path, 'r') as infile:\n",
        "        html_data = infile.read()\n",
        "    soup = BeautifulSoup(html_data, 'html.parser')\n",
        "    p_tags = soup.find_all('p')\n",
        "    wikified_doc = []\n",
        "    for p_tag in p_tags:\n",
        "        wikified_doc.append(wikify(p_tag.text))\n",
        "    with open(out_path, 'w') as outfile:\n",
        "        json.dump(wikified_doc, outfile)   \n",
        "\n",
        "def get_enhanced_html(path):\n",
        "    json_path = get_out_path(path)\n",
        "    if os.path.exists(json_path):\n",
        "        with open(json_path) as f:\n",
        "            Wikis = json.load(f) \n",
        "    else:\n",
        "        wikify_html(path)\n",
        "        with open(json_path) as f:\n",
        "            Wikis = json.load(f)         \n",
        "    with open(path, \"r\") as infile: \n",
        "        html_file = infile.read()\n",
        "    newsoup = BeautifulSoup(html_file, 'html.parser')\n",
        "    p_tags = newsoup.find_all('p')\n",
        "    p_tags_index_tuples = [(p_tag, i) for i, p_tag in enumerate(p_tags)]    \n",
        "    annotations_and_num = [(wiki, len(wiki['annotations'])) for i, wiki in enumerate(Wikis)]    \n",
        "    annotated_paragraph_text = []\n",
        "    for annot in annotations_and_num:\n",
        "        if annot[1]==True:\n",
        "            annotated_paragraph_text.append(embed_links(annot[0]).data)\n",
        "        else:\n",
        "            annotated_paragraph_text.append(HTML(build_doc(annot[0])).data)\n",
        "    enhanced_paragraph = []\n",
        "    for each_par in annotated_paragraph_text:\n",
        "        html_par = \"<p>\"+each_par+\"</p>\"\n",
        "        enhanced_paragraph.append(BeautifulSoup(html_par, 'html.parser')) \n",
        "    i = 0\n",
        "    for p in newsoup.find_all('p'):\n",
        "        n_para = enhanced_paragraph[i]\n",
        "        if p.string:\n",
        "            p.string.replace_with(n_para)\n",
        "        else:\n",
        "            continue\n",
        "        i += 1\n",
        "        if i == len(p_tags)-1:\n",
        "            break\n",
        "        else:\n",
        "            continue\n",
        "    with open(path[:-4]+\"Enhanced.html\", 'wb') as f:\n",
        "        f.write(newsoup.renderContents())\n",
        "        \n",
        "    with open(path[:-4]+\"Enhanced.html\", \"r\") as f: \n",
        "        enhanced_html_file = f.read()\n",
        "        \n",
        "def enhanced_book(book_id):\n",
        "    path = book_id+\".htm\"\n",
        "    if os.path.exists(path):\n",
        "        pass\n",
        "    else:\n",
        "        Gutentag(book_id)\n",
        "    get_enhanced_html(path)\n",
        "    return  IFrame(path[:-4]+\"_Enhanced.html\", width=800, height=200)\n",
        "\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hAyF5pIXRn5N",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "outputId": "e5d98976-e8c6-41fe-8275-20ff70fecad5"
      },
      "source": [
        "enhanced_book(\"19033\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "        <iframe\n",
              "            width=\"800\"\n",
              "            height=\"200\"\n",
              "            src=\"19033_Enhanced.html\"\n",
              "            frameborder=\"0\"\n",
              "            allowfullscreen\n",
              "        ></iframe>\n",
              "        "
            ],
            "text/plain": [
              "<IPython.lib.display.IFrame at 0x7f2e3bdab668>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    }
  ]
}
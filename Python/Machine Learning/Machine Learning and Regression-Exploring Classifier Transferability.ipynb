{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine learning and regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring Classifier Transferability\n",
    "### Data sets\n",
    "\n",
    "__Data Set 1:__ This dataset is based around examining real vs fake reviews. These opinion SPAM reviews were produced by a researcher named Myle Ott. In addition to collecting real reviews on hotels from the web and TripAdvisor, Ott et al. ran Amazon Mechanical Turk surveys to have real people write both positive and negative fake reviews of the hotels:\n",
    "\n",
    "- http://myleott.com/op-spam.html\n",
    "\n",
    "The goal with the data set was to train computers to detect which reviews were real vs. fake. These are provided in the following nested file structure:\n",
    "\n",
    "- `./data/op_spam_v1.4/negative_polarity/deceptive_from_MTurk/fold[1-5]/*.txt`\n",
    "- `./data/op_spam_v1.4/positive_polarity/deceptive_from_MTurk/fold[1-5]/*.txt`\n",
    "- `./data/op_spam_v1.4/negative_polarity/truthful_from_Web/fold[1-5]/*.txt`\n",
    "- `./data/op_spam_v1.4/positive_polarity/truthful_from_TripAdvisor/fold[1-5]/*.txt`\n",
    "\n",
    "__Data Set 2:__ The purpose of this code is to train an Opinion SPAM classifier on the _curated_ __Data Set 1__, and apply it to get an idea of how prolific SPAM is on this completely different, _real-world_ hotel [booking website's](booking.com) data. The data from this website live in the assignment's data directory, too:\n",
    "\n",
    "- `./data/Hotel_Reviews.csv`\n",
    "    \n",
    "and were taken from [Kaggle](https://www.kaggle.com/jiashenliu/515k-hotel-reviews-data-in-europe)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to load the Op SPAM data using `sklearn`. As the data is across multiple files, I had to build a a full list of all the different review files in the data set using `glob` module's `.glob(regex)` method to output a list of all `all_files` matching the provided `regex` pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['./data/op_spam_v1.4\\\\negative_polarity\\\\deceptive_from_MTurk\\\\fold1\\\\d_hilton_1.txt',\n",
      " './data/op_spam_v1.4\\\\negative_polarity\\\\deceptive_from_MTurk\\\\fold1\\\\d_hilton_10.txt',\n",
      " './data/op_spam_v1.4\\\\negative_polarity\\\\deceptive_from_MTurk\\\\fold1\\\\d_hilton_11.txt',\n",
      " './data/op_spam_v1.4\\\\negative_polarity\\\\deceptive_from_MTurk\\\\fold1\\\\d_hilton_12.txt',\n",
      " './data/op_spam_v1.4\\\\negative_polarity\\\\deceptive_from_MTurk\\\\fold1\\\\d_hilton_13.txt']\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "from pprint import pprint\n",
    "all_files = glob.glob('./data/op_spam_v1.4/*_polarity/*/*/*.txt')\n",
    "pprint(all_files[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is based around supervised learning, but the data does not have implict labels. To construct the labels from the file names, I used a regex match on `all_files`. As this is doing sentiment classification, I utilized the word 'positive_polarity' in the file path to indicate a positve label (of value `1`) and otherwise use a negative label (value `0`). Store these values in a `np.array()` called `labels`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800\n",
      "800\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import collections\n",
    "mask_files = []\n",
    "\n",
    "for t in all_files:\n",
    "    #print(t.find('negative'))\n",
    "    if re.search('positive',t):\n",
    "        k = 1\n",
    "        mask_files.append(k)\n",
    "\n",
    "    else:\n",
    "        k = 0\n",
    "        mask_files.append(k)\n",
    "\n",
    "    \n",
    "import numpy as np\n",
    "labels = np.array(mask_files)\n",
    "\n",
    "print(len(labels)- sum(labels)) # count of 0s\n",
    "print(sum(labels)) #count of 1s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, I `import`ed `sklearn`'s TDM-maker `CountVectorizer` from `sklearn.feature_extraction.text`. Then I initialized an instance of \n",
    "- `CountVectorizer(input = 'filename')` \n",
    "\n",
    "and called `vectorizer`, to apply its `.fit()` and `.transform()` methods to `all_files`. The end result produces a `TDM`.\n",
    "\n",
    "I then converted the matrix to a dense representation using `TDM.toarray()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1600, 9571)\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(input = 'filename')\n",
    "\n",
    "TDM = vectorizer.fit_transform(all_files)\n",
    "TDM = TDM.toarray()\n",
    "print(TDM.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, I used `train_test_split` to split the `TDM` and `labels` into $75\\%$ training and $25\\%$ test sets, importing the function from `sklearn.model_selection`. Also, I determined that I wanted to use `random_state = 0` as my random state as this will come up later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, test, train_labels, test_labels = train_test_split(TDM,labels,test_size=0.25, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I `import`ed, initialize, and `.fit()` a series of binary classifiers with `sklearn` on the training data split to compare their precision, recall, and F1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "newton-cg\n",
      "0.9225\n",
      "[1 0 1 1 1 0]\n",
      "[[4.12962150e-02 9.58703785e-01]\n",
      " [9.97808290e-01 2.19170995e-03]\n",
      " [8.98474284e-02 9.10152572e-01]\n",
      " [2.52990737e-05 9.99974701e-01]\n",
      " [5.81132751e-04 9.99418867e-01]\n",
      " [9.49928921e-01 5.00710788e-02]]\n",
      "Precision, recall, and F1 were:\n",
      "0.9128205128205128\n",
      "0.9270833333333334\n",
      "0.9198966408268734\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jlbochenek\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lbfgs\n",
      "0.9225\n",
      "[1 0 1 1 1 0]\n",
      "[[4.12494446e-02 9.58750555e-01]\n",
      " [9.97801105e-01 2.19889502e-03]\n",
      " [8.99812843e-02 9.10018716e-01]\n",
      " [2.51895718e-05 9.99974810e-01]\n",
      " [5.80446675e-04 9.99419553e-01]\n",
      " [9.49977673e-01 5.00223272e-02]]\n",
      "Precision, recall, and F1 were:\n",
      "0.9128205128205128\n",
      "0.9270833333333334\n",
      "0.9198966408268734\n",
      "liblinear\n",
      "0.9225\n",
      "[1 0 1 1 1 0]\n",
      "[[4.05146895e-02 9.59485310e-01]\n",
      " [9.97719456e-01 2.28054411e-03]\n",
      " [9.19005313e-02 9.08099469e-01]\n",
      " [2.41680569e-05 9.99975832e-01]\n",
      " [5.67617652e-04 9.99432382e-01]\n",
      " [9.48333526e-01 5.16664740e-02]]\n",
      "Precision, recall, and F1 were:\n",
      "0.9128205128205128\n",
      "0.9270833333333334\n",
      "0.9198966408268734\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jlbochenek\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sag\n",
      "0.9275\n",
      "[1 0 1 1 1 0]\n",
      "[[0.23455944 0.76544056]\n",
      " [0.97222867 0.02777133]\n",
      " [0.14620265 0.85379735]\n",
      " [0.00185253 0.99814747]\n",
      " [0.00331635 0.99668365]\n",
      " [0.89274615 0.10725385]]\n",
      "Precision, recall, and F1 were:\n",
      "0.9179487179487179\n",
      "0.9322916666666666\n",
      "0.9250645994832041\n",
      "saga\n",
      "0.9275\n",
      "[1 0 1 1 1 0]\n",
      "[[0.33750908 0.66249092]\n",
      " [0.92871187 0.07128813]\n",
      " [0.17840061 0.82159939]\n",
      " [0.00779389 0.99220611]\n",
      " [0.00815339 0.99184661]\n",
      " [0.90116798 0.09883202]]\n",
      "Precision, recall, and F1 were:\n",
      "0.9179487179487179\n",
      "0.9322916666666666\n",
      "0.9250645994832041\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jlbochenek\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "\n",
    "classifiers = ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']\n",
    "models = []\n",
    "\n",
    "for clf in classifiers:\n",
    "    Logistic_classifier = LogisticRegression(solver=clf)\n",
    "    Logistic_classifier.fit(train, train_labels)\n",
    "    score = Logistic_classifier.score(test, test_labels)\n",
    "    predictions = Logistic_classifier.predict(test)\n",
    "    models.append([clf, score, predictions, Logistic_classifier.predict_proba(test), precision_score(predictions, test_labels),recall_score(predictions, test_labels),f1_score(predictions, test_labels)])\n",
    "    print(clf)\n",
    "    print(score)\n",
    "    print(predictions[:6])\n",
    "    print(Logistic_classifier.predict_proba(test)[:6])\n",
    "    print(\"Precision, recall, and F1 were:\")\n",
    "    print(precision_score(predictions, test_labels))\n",
    "    print(recall_score(predictions, test_labels))\n",
    "    print(f1_score(predictions, test_labels))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy is a result of a confusion matrix that says predicted/classified vs actual which in this case is 92.5%, which can be good if the ones you are misclassifying are not dangerous to misclassify. Precision is (true positive/(true positive+false positive)) or (true positive / total predicted positive), so its saying how many of the total you called positive were actually actually positive. This is 91.7% which is pretty good, and important to calculate when the cost of false negatives are high. Recall is (true positive/(true positive + false negative)) or actual positive = (true positive + false negative) so knowing that ours is 92.7% we know that our false negatives are relatively low. $F_1$ is 2X((precisionXrecall)/(precision+recall)) thus better when you want a balance between precision and recall and there are uneven class distributions (i.e., larger numbers of actual positves or negatives) however for this data we have the same number of class distributions.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9179487179487179, 0.9322916666666666, 0.9250645994832041]\n",
      "0.9275\n"
     ]
    }
   ],
   "source": [
    "print(models[4][-3:]) #precision, recall, F1\n",
    "print(models[4][1]) #accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As I previously mentioned, I also wanted to see how well this sentiment polarity classifier does on a different data set:\n",
    "\n",
    "- `./data/Hotel_Reviews.csv`\n",
    "\n",
    "which was hosted on a Kaggle competition, but came from from Booking.com:\n",
    "\n",
    "- https://www.kaggle.com/jiashenliu/515k-hotel-reviews-data-in-europe\n",
    "\n",
    "There's a decent description of the data there, where it seems a customer can comment with positive and negative reviews, in parallel. To get started, I loaded these data in with pandas and provided the information about this dataset below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "hotels = pd.read_csv('./data/Hotel_Reviews.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hotel_Address',\n",
       " 'Additional_Number_of_Scoring',\n",
       " 'Review_Date',\n",
       " 'Average_Score',\n",
       " 'Hotel_Name',\n",
       " 'Reviewer_Nationality',\n",
       " 'Negative_Review',\n",
       " 'Review_Total_Negative_Word_Counts',\n",
       " 'Total_Number_of_Reviews',\n",
       " 'Positive_Review',\n",
       " 'Review_Total_Positive_Word_Counts',\n",
       " 'Total_Number_of_Reviews_Reviewer_Has_Given',\n",
       " 'Reviewer_Score',\n",
       " 'Tags',\n",
       " 'days_since_review',\n",
       " 'lat',\n",
       " 'lng']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(hotels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Hotel_Address</th>\n",
       "      <th>Additional_Number_of_Scoring</th>\n",
       "      <th>Review_Date</th>\n",
       "      <th>Average_Score</th>\n",
       "      <th>Hotel_Name</th>\n",
       "      <th>Reviewer_Nationality</th>\n",
       "      <th>Negative_Review</th>\n",
       "      <th>Review_Total_Negative_Word_Counts</th>\n",
       "      <th>Total_Number_of_Reviews</th>\n",
       "      <th>Positive_Review</th>\n",
       "      <th>Review_Total_Positive_Word_Counts</th>\n",
       "      <th>Total_Number_of_Reviews_Reviewer_Has_Given</th>\n",
       "      <th>Reviewer_Score</th>\n",
       "      <th>Tags</th>\n",
       "      <th>days_since_review</th>\n",
       "      <th>lat</th>\n",
       "      <th>lng</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>s Gravesandestraat 55 Oost 1092 AA Amsterdam ...</td>\n",
       "      <td>194</td>\n",
       "      <td>8/3/2017</td>\n",
       "      <td>7.7</td>\n",
       "      <td>Hotel Arena</td>\n",
       "      <td>Russia</td>\n",
       "      <td>I am so angry that i made this post available...</td>\n",
       "      <td>397</td>\n",
       "      <td>1403</td>\n",
       "      <td>Only the park outside of the hotel was beauti...</td>\n",
       "      <td>11</td>\n",
       "      <td>7</td>\n",
       "      <td>2.9</td>\n",
       "      <td>[' Leisure trip ', ' Couple ', ' Duplex Double...</td>\n",
       "      <td>0 days</td>\n",
       "      <td>52.360576</td>\n",
       "      <td>4.915968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>s Gravesandestraat 55 Oost 1092 AA Amsterdam ...</td>\n",
       "      <td>194</td>\n",
       "      <td>8/3/2017</td>\n",
       "      <td>7.7</td>\n",
       "      <td>Hotel Arena</td>\n",
       "      <td>Ireland</td>\n",
       "      <td>No Negative</td>\n",
       "      <td>0</td>\n",
       "      <td>1403</td>\n",
       "      <td>No real complaints the hotel was great great ...</td>\n",
       "      <td>105</td>\n",
       "      <td>7</td>\n",
       "      <td>7.5</td>\n",
       "      <td>[' Leisure trip ', ' Couple ', ' Duplex Double...</td>\n",
       "      <td>0 days</td>\n",
       "      <td>52.360576</td>\n",
       "      <td>4.915968</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       Hotel_Address  \\\n",
       "0   s Gravesandestraat 55 Oost 1092 AA Amsterdam ...   \n",
       "1   s Gravesandestraat 55 Oost 1092 AA Amsterdam ...   \n",
       "\n",
       "   Additional_Number_of_Scoring Review_Date  Average_Score   Hotel_Name  \\\n",
       "0                           194    8/3/2017            7.7  Hotel Arena   \n",
       "1                           194    8/3/2017            7.7  Hotel Arena   \n",
       "\n",
       "  Reviewer_Nationality                                    Negative_Review  \\\n",
       "0              Russia    I am so angry that i made this post available...   \n",
       "1             Ireland                                         No Negative   \n",
       "\n",
       "   Review_Total_Negative_Word_Counts  Total_Number_of_Reviews  \\\n",
       "0                                397                     1403   \n",
       "1                                  0                     1403   \n",
       "\n",
       "                                     Positive_Review  \\\n",
       "0   Only the park outside of the hotel was beauti...   \n",
       "1   No real complaints the hotel was great great ...   \n",
       "\n",
       "   Review_Total_Positive_Word_Counts  \\\n",
       "0                                 11   \n",
       "1                                105   \n",
       "\n",
       "   Total_Number_of_Reviews_Reviewer_Has_Given  Reviewer_Score  \\\n",
       "0                                           7             2.9   \n",
       "1                                           7             7.5   \n",
       "\n",
       "                                                Tags days_since_review  \\\n",
       "0  [' Leisure trip ', ' Couple ', ' Duplex Double...            0 days   \n",
       "1  [' Leisure trip ', ' Couple ', ' Duplex Double...            0 days   \n",
       "\n",
       "         lat       lng  \n",
       "0  52.360576  4.915968  \n",
       "1  52.360576  4.915968  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hotels.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One difficulty is that sometimes, a reviewer won't leave a positive or negative review in one of the categories. However, what's left is not a conventional N/A or anything. Refering back to the data dictionary:\n",
    "\n",
    "- https://www.kaggle.com/jiashenliu/515k-hotel-reviews-data-in-europe\n",
    "\n",
    "So if there is no positive review, it says No Positive, and similarly there is no negative review it says No Negative. This has to be dealt with. To do this I made a list of the non-null reviews with a parallel list of labels, again 1 for positive reviews and 0 for negative reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "Hotel_labels = []\n",
    "Hotel_review = []\n",
    "for row in hotels['Negative_Review']:\n",
    "    if row == \"No Negative\":\n",
    "        pass\n",
    "    else:\n",
    "        i = 0\n",
    "        Hotel_labels.append(i)\n",
    "        Hotel_review.append(row)\n",
    "for row in hotels['Positive_Review']:\n",
    "    if row == \"No Positive\":\n",
    "        pass\n",
    "    else:\n",
    "        i = 1\n",
    "        Hotel_labels.append(i)\n",
    "        Hotel_review.append(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are more positive reviews than negative reviews (480k compared to 388k, or 55% compared to 45%), but this is not a high level of class imbalanced, but it can be corrected by sampling if desired."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4470148909686045\n",
      "0.5529851090313955\n",
      "[(' The ac was useless It was a hot week in vienna and it only gave more hot air', 0), (' I was in 3rd floor It didn t work Free Wife ', 0), (' Only the park outside of the hotel was beautiful ', 1), (' No real complaints the hotel was great great location surroundings rooms amenities and service Two recommendations however firstly the staff upon check in are very confusing regarding deposit payments and the staff offer you upon checkout to refund your original payment and you can make a new one Bit confusing Secondly the on site restaurant is a bit lacking very well thought out and excellent quality food for anyone of a vegetarian or vegan background but even a wrap or toasted sandwich option would be great Aside from those minor minor things fantastic spot and will be back when i return to Amsterdam ', 1)]\n"
     ]
    }
   ],
   "source": [
    "print((len(Hotel_review)-sum(Hotel_labels))/len(Hotel_review)) #number/percent of zeros\n",
    "print(sum(Hotel_labels)/len(Hotel_review)) #number/percent of ones\n",
    "\n",
    "testing123=zip(Hotel_review,Hotel_labels)\n",
    "print(list(testing123)[387846:387850])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `CountVectorizer()` again&mdash; I then created a TDM for the new hotel data. To do so I used the same initialized vectorizer from earlier, this is what makes it an exercise in classifier trasferability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer.input = 'content'\n",
    "TDM2 = vectorizer.transform(Hotel_review)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When appling the classifier to this new, Booking.com TDM, I also computed the accuracy, precision, recall, and $F_1$. As discussed earlier the class imbalance means that we should look at the F1, which is around 81% for each model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "newton-cg\n",
      "0.7883684477433037\n",
      "[1 0 1 1 1 0]\n",
      "[[1.61442581e-01 8.38557419e-01]\n",
      " [5.33155204e-01 4.66844796e-01]\n",
      " [2.21882104e-01 7.78117896e-01]\n",
      " [3.53371636e-01 6.46628364e-01]\n",
      " [3.99005573e-04 9.99600994e-01]\n",
      " [5.34914439e-01 4.65085561e-01]]\n",
      "Precision, recall, and F1 were:\n",
      "0.8518240754628935\n",
      "0.7842451303060293\n",
      "0.8166389058649188\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jlbochenek\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lbfgs\n",
      "0.7887003826471809\n",
      "[1 0 1 1 1 0]\n",
      "[[1.62130203e-01 8.37869797e-01]\n",
      " [5.34498945e-01 4.65501055e-01]\n",
      " [2.22848508e-01 7.77151492e-01]\n",
      " [3.54122477e-01 6.45877523e-01]\n",
      " [3.97728000e-04 9.99602272e-01]\n",
      " [5.36314321e-01 4.63685679e-01]]\n",
      "Precision, recall, and F1 were:\n",
      "0.8508074596270186\n",
      "0.7852127569579562\n",
      "0.8166951291208902\n",
      "liblinear\n",
      "0.7931400119865382\n",
      "[1 0 1 1 1 0]\n",
      "[[1.70012340e-01 8.29987660e-01]\n",
      " [5.50046837e-01 4.49953163e-01]\n",
      " [2.34190074e-01 7.65809926e-01]\n",
      " [3.64255036e-01 6.35744964e-01]\n",
      " [3.92536915e-04 9.99607463e-01]\n",
      " [5.52288419e-01 4.47711581e-01]]\n",
      "Precision, recall, and F1 were:\n",
      "0.835833208339583\n",
      "0.7994086424279133\n",
      "0.8172152517516702\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jlbochenek\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sag\n",
      "0.8220921119358259\n",
      "[1 0 1 1 1 0]\n",
      "[[0.24029359 0.75970641]\n",
      " [0.58612999 0.41387001]\n",
      " [0.29471479 0.70528521]\n",
      " [0.31779801 0.68220199]\n",
      " [0.00344781 0.99655219]\n",
      " [0.50384282 0.49615718]]\n",
      "Precision, recall, and F1 were:\n",
      "0.9064380114327617\n",
      "0.7990157191126781\n",
      "0.8493437336519016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jlbochenek\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saga\n",
      "0.7992347056382831\n",
      "[1 0 1 1 1 1]\n",
      "[[0.27653388 0.72346612]\n",
      " [0.58240944 0.41759056]\n",
      " [0.32673295 0.67326705]\n",
      " [0.30332667 0.69667333]\n",
      " [0.00645372 0.99354628]\n",
      " [0.49036772 0.50963228]]\n",
      "Precision, recall, and F1 were:\n",
      "0.9323283835808209\n",
      "0.7595104268491365\n",
      "0.8370929005903082\n"
     ]
    }
   ],
   "source": [
    "train2, test2, train_labels2, test_labels2 = train_test_split(TDM2,Hotel_labels,test_size=0.25, random_state=0)\n",
    "classifiers = ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']\n",
    "models2 = []\n",
    "\n",
    "for clf in classifiers:\n",
    "    Logistic_classifier = LogisticRegression(solver=clf)\n",
    "    Logistic_classifier.fit(train, train_labels)\n",
    "    score = Logistic_classifier.score(test2, test_labels2)\n",
    "    predictions = Logistic_classifier.predict(test2)\n",
    "    models2.append([clf, score, predictions, Logistic_classifier.predict_proba(test2), precision_score(predictions, test_labels2),recall_score(predictions, test_labels2),f1_score(predictions, test_labels2)])\n",
    "    print(clf)\n",
    "    print(score)\n",
    "    print(predictions[:6])\n",
    "    print(Logistic_classifier.predict_proba(test2)[:6])\n",
    "    print(\"Precision, recall, and F1 were:\")\n",
    "    print(precision_score(predictions, test_labels2))\n",
    "    print(recall_score(predictions, test_labels2))\n",
    "    print(f1_score(predictions, test_labels2))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing these these results with the results from earlier, the recall is much worse across the board, the while the precision is only a little worse. Thus the $F_1$ are lower overall."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I was interested in going back and looking at the original dataset and rebuilding it to look at whether or not they are true (using the deceptive label on some files), with deceptive labeled as 1 and files not labeled as deceptive marked as 0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800\n",
      "800\n"
     ]
    }
   ],
   "source": [
    "mask_files =[]\n",
    "\n",
    "for t in all_files:\n",
    "    #print(t.find('negative'))\n",
    "    if re.search('deceptive',t):\n",
    "        k = 1\n",
    "        mask_files.append(k)\n",
    "\n",
    "    else:\n",
    "        k = 0\n",
    "        mask_files.append(k)\n",
    "\n",
    "labels = np.array(mask_files)\n",
    "\n",
    "print(len(labels)- sum(labels)) # count of 0s\n",
    "print(sum(labels)) #count of 1s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this I had to train a new classifier on _all_ of the Opinion SPAM labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "newton-cg\n",
      "0.855\n",
      "[1 1 0 1 1 0]\n",
      "[[2.26589458e-01 7.73410542e-01]\n",
      " [6.42914939e-05 9.99935709e-01]\n",
      " [6.55221297e-01 3.44778703e-01]\n",
      " [4.59268999e-03 9.95407310e-01]\n",
      " [3.24572157e-01 6.75427843e-01]\n",
      " [9.99819875e-01 1.80125010e-04]]\n",
      "Precision, recall, and F1 were:\n",
      "0.8469387755102041\n",
      "0.8556701030927835\n",
      "0.8512820512820514\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jlbochenek\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lbfgs\n",
      "0.855\n",
      "[1 1 0 1 1 0]\n",
      "[[2.27332236e-01 7.72667764e-01]\n",
      " [6.44517766e-05 9.99935548e-01]\n",
      " [6.57542922e-01 3.42457078e-01]\n",
      " [4.43444580e-03 9.95565554e-01]\n",
      " [3.32764753e-01 6.67235247e-01]\n",
      " [9.99826871e-01 1.73128848e-04]]\n",
      "Precision, recall, and F1 were:\n",
      "0.8469387755102041\n",
      "0.8556701030927835\n",
      "0.8512820512820514\n",
      "liblinear\n",
      "0.85\n",
      "[1 1 0 1 1 0]\n",
      "[[2.29299554e-01 7.70700446e-01]\n",
      " [6.37693281e-05 9.99936231e-01]\n",
      " [6.50379387e-01 3.49620613e-01]\n",
      " [4.89342760e-03 9.95106572e-01]\n",
      " [3.29705880e-01 6.70294120e-01]\n",
      " [9.99821697e-01 1.78302973e-04]]\n",
      "Precision, recall, and F1 were:\n",
      "0.8367346938775511\n",
      "0.8541666666666666\n",
      "0.845360824742268\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jlbochenek\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sag\n",
      "0.85\n",
      "[1 1 0 1 1 0]\n",
      "[[0.27528864 0.72471136]\n",
      " [0.00238204 0.99761796]\n",
      " [0.65144443 0.34855557]\n",
      " [0.01148668 0.98851332]\n",
      " [0.32291663 0.67708337]\n",
      " [0.96409129 0.03590871]]\n",
      "Precision, recall, and F1 were:\n",
      "0.8418367346938775\n",
      "0.8505154639175257\n",
      "0.846153846153846\n",
      "saga\n",
      "0.8425\n",
      "[1 1 0 1 1 0]\n",
      "[[0.32270378 0.67729622]\n",
      " [0.00647821 0.99352179]\n",
      " [0.64522627 0.35477373]\n",
      " [0.02588351 0.97411649]\n",
      " [0.30724531 0.69275469]\n",
      " [0.85130072 0.14869928]]\n",
      "Precision, recall, and F1 were:\n",
      "0.8418367346938775\n",
      "0.8375634517766497\n",
      "0.8396946564885496\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jlbochenek\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "train3, test3, train_labels3, test_labels3 = train_test_split(TDM,labels,test_size=0.25, random_state=0)\n",
    "\n",
    "classifiers = ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']\n",
    "models3 = []\n",
    "\n",
    "for clf in classifiers:\n",
    "    Logistic_classifier = LogisticRegression(solver=clf)\n",
    "    Logistic_classifier.fit(train3, train_labels3)\n",
    "    score = Logistic_classifier.score(test3, test_labels3)\n",
    "    predictions = Logistic_classifier.predict(test3)\n",
    "    models3.append([clf, score, predictions, Logistic_classifier.predict_proba(test3), precision_score(predictions, test_labels3),recall_score(predictions, test_labels3),f1_score(predictions, test_labels3)])\n",
    "    print(clf)\n",
    "    print(score)\n",
    "    print(predictions[:6])\n",
    "    print(Logistic_classifier.predict_proba(test)[:6])\n",
    "    print(\"Precision, recall, and F1 were:\")\n",
    "    print(precision_score(predictions, test_labels3))\n",
    "    print(recall_score(predictions, test_labels3))\n",
    "    print(f1_score(predictions, test_labels3))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running this new classifier that I just trained against the new hotel reviews data set. I set the classification threshold at $0.5$ to find SPAM reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "newton-cg\n",
      "0.44417039325065694\n",
      "[1 0 0 0 0 0]\n",
      "[[0.35642289 0.64357711]\n",
      " [0.76411942 0.23588058]\n",
      " [0.66325492 0.33674508]\n",
      " [0.87126427 0.12873573]\n",
      " [0.9753977  0.0246023 ]\n",
      " [0.63757917 0.36242083]]\n",
      "Precision, recall, and F1 were:\n",
      "0.09638684732430045\n",
      "0.4882032667876588\n",
      "0.16098929011336194\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jlbochenek\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lbfgs\n",
      "0.4439721543497303\n",
      "[1 0 0 0 0 0]\n",
      "[[0.35436375 0.64563625]\n",
      " [0.76410586 0.23589414]\n",
      " [0.65999324 0.34000676]\n",
      " [0.8698751  0.1301249 ]\n",
      " [0.97565119 0.02434881]\n",
      " [0.6340093  0.3659907 ]]\n",
      "Precision, recall, and F1 were:\n",
      "0.09809509524523774\n",
      "0.4875341671498385\n",
      "0.1633275986458738\n",
      "liblinear\n",
      "0.4418975612004979\n",
      "[1 0 0 0 0 0]\n",
      "[[0.33150996 0.66849004]\n",
      " [0.73822121 0.26177879]\n",
      " [0.63347064 0.36652936]\n",
      " [0.85954438 0.14045562]\n",
      " [0.97588647 0.02411353]\n",
      " [0.60424642 0.39575358]]\n",
      "Precision, recall, and F1 were:\n",
      "0.1183190840457977\n",
      "0.4821392190152801\n",
      "0.19000909966812973\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jlbochenek\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sag\n",
      "0.4367295191554101\n",
      "[1 0 0 0 0 0]\n",
      "[[0.41420406 0.58579594]\n",
      " [0.65319014 0.34680986]\n",
      " [0.58794645 0.41205355]\n",
      " [0.78645479 0.21354521]\n",
      " [0.96428004 0.03571996]\n",
      " [0.56748047 0.43251953]]\n",
      "Precision, recall, and F1 were:\n",
      "0.08687065646717664\n",
      "0.45280806150371367\n",
      "0.14577463311636102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jlbochenek\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saga\n",
      "0.4338619704024711\n",
      "[1 0 0 0 0 0]\n",
      "[[0.43568092 0.56431908]\n",
      " [0.6151047  0.3848953 ]\n",
      " [0.56332884 0.43667116]\n",
      " [0.73669076 0.26330924]\n",
      " [0.94870189 0.05129811]\n",
      " [0.54290986 0.45709014]]\n",
      "Precision, recall, and F1 were:\n",
      "0.08933719980667633\n",
      "0.4423402236250361\n",
      "0.14865192764986862\n"
     ]
    }
   ],
   "source": [
    "classifiers = ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']\n",
    "models4 = []\n",
    "\n",
    "for clf in classifiers:\n",
    "    Logistic_classifier = LogisticRegression(solver=clf)\n",
    "    Logistic_classifier.fit(train3, train_labels3)\n",
    "    score = Logistic_classifier.score(test2, test_labels2) #but these are the positive and negative labels...? How is this working as a dependant variable for spam detection?\n",
    "    predictions = Logistic_classifier.predict(test2)\n",
    "    models4.append([clf, score, predictions, test2, Logistic_classifier.predict_proba(test2), precision_score(predictions, test_labels2),recall_score(predictions, test_labels2),f1_score(predictions, test_labels2)])\n",
    "    print(clf)\n",
    "    print(score)\n",
    "    print(predictions[:6])\n",
    "    print(Logistic_classifier.predict_proba(test2)[:6])\n",
    "    print(\"Precision, recall, and F1 were:\")\n",
    "    print(precision_score(predictions, test_labels2))\n",
    "    print(recall_score(predictions, test_labels2))\n",
    "    print(f1_score(predictions, test_labels2))  \n",
    "    \n",
    "# threshold of  0.5? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0.35642289, 0.64357711],\n",
       "        [0.76411942, 0.23588058],\n",
       "        [0.66325492, 0.33674508],\n",
       "        ...,\n",
       "        [0.73626411, 0.26373589],\n",
       "        [0.97437101, 0.02562899],\n",
       "        [0.95579896, 0.04420104]]),)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models4[0][4],"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What I found is that this classifier does not predict well, so this is not good indicator for the booking.com's dataset for determining truthful vs paid reviews. To examine this further, I sorted the Booking.com's reviews by their predicted probabilities from high to low. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#just selecting one of the classifers to sort\n",
    "\n",
    "Sorted_values = sorted(zip(models4[0][4][:,1],Hotel_review))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2.6204199698427492e-11, ' The room was really small With my suitcase open I had to climb over the bed to g et to the bathroom ')\n"
     ]
    }
   ],
   "source": [
    "print(Sorted_values[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As there are no SPAM labels for the Booking.com data, I inspected the first few most and least spammy reviews. To the human eye there is not too much difference between the 'most' and 'least' spammy reviews. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.9996488261298903, ' The Deluxe Room was pricey and extremely small It was so small that a big suitcase couldn t be moved around The bed was too narrow for an average sized adult to roll from one side to the other ')\n"
     ]
    }
   ],
   "source": [
    "print(Sorted_values[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.9987647855698705, ' Not enough lifts we were on 7th floor took 15 mins from room to reception Saturday morning not much better late afternoon Too many light switches 13 in 1 room ')\n"
     ]
    }
   ],
   "source": [
    "print(Sorted_values[-3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3.903642702145739e-10, ' The room was a little snug but we went for the cheapest option so that was to be expected Plus the aircon wasn t too good and didn t come with instructions luckily we weren t too hot ')\n"
     ]
    }
   ],
   "source": [
    "print(Sorted_values[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

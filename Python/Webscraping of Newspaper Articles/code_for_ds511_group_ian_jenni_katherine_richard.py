# -*- coding: utf-8 -*-
"""Code for DS511 Group Ian Jenni Katherine Richard.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1VK1temnoItR37vmyul8_KuWVNAP_96vn

# **Webscraping a Google News Dataset**
DSCI 511 Group Project 
Jenni Bochenek, Ian Fitzsimmons, Richard Hong, Katherine Houseman

#Our Team
In order of level of contribution:

**Jenni Bochenek:** I have a BS in Psychology, focusing on the Psychobiology of Addictions  with a minor in Biology, from Purdue University (Indianapolis Campus), and a MS from New Mexico Highlands Univeristy in Psychology. Recently I completed a 10 course specialization in Data Science from John Hopkins University on Coursera. The courses were taught in R, not Python. I currently work at Educational Testing Service (ETS) in the Academic to Career (AtC) Research Center as a Research Associate. My interests are in education, particularly Higher Education, but I also have a strong background in biology and pharmacology. **Skills:** Coding, data managment, and data cleaning **Contributions:** Code for webscraping rss feeds

**Ian Fitzsimmons**: I have a BS in Aerospace Engineering from Penn State University. I have been working at Lockheed Martin full time since graduating in 2016. At Lockheed Martin I work on software development and testing. Software is written mostly in C++ and most of the analysis is done in Python.  **Skills:** Coding **Contributions:** Code for analysis (get keywords by hour and by day). 

**Richard Hong**: I have a BA in Music, focusing on Music Theory, from City College of New York (CUNY).  I have experience with SAS programming with SAS base certification. I’ve been serving as a worship leader and music director for more than 10 years at Antioch Church of Philadelphia. **Skills:** Data structures **Contributions:** Cron coding

**Katherine Houseman**: I have a social sciences background: BA in Anthropology (University of Pennsylvania), MA in Teaching English as a Second Language (SIT Graduate Institute), and MS in Global and International Education (Drexel). I’ve been an ESL teacher for 10 years, the last six of which have been at Drexel. **Skills:** Data visualization, writing **Contributions:** writing and organization of report

#Our Project

##Introduction
The ubiquity of news, from social media to websites to news aggregators, in addition to the emergence of "fake" news, has reinvigorated fundamental questions in journalism about how events are constituted and packaged as news stories (often called "news values") and how stories affect people's perceptions of the world. Our data science project is informed by studies of news values (Galtung & Ruge, 1965; Harcup & O'Neill, 2000, 2009); sentiment analysis (Berger & Milkman, 2012); algorithmic judgment (Carlson, 2015) and news bias (Hamborg,  Meuschke, & Gipp, 2018). For our project, we wanted to design code that would generate a dataset of news articles from Google News which could be used for further study, possibly in one of the areas above. These data could be useful to researchers in the fields of journalism, linguistics, and political science, among others. 

We extracted information from google news using their RSS feeds from: 

  **google headlines**: https://news.google.com/news/rss/
 
  **google world**: https://news.google.com/news/rss/headlines/section/topic/WORLD
  
  **google business**: https://news.google.com/news/rss/headlines/section/topic/BUSINESS
  
  **google nation**: https://news.google.com/news/rss/headlines/section/topic/NATION
  
  **google health**: https://news.google.com/news/rss/headlines/section/topic/HEALTH
  
  **google sports**: https://news.google.com/news/rss/headlines/section/topic/SPORTS
  
 **google tech**: https://news.google.com/news/rss/headlines/section/topic/TECHNOLOGY
 
  **google entertainment**: https://news.google.com/news/rss/headlines/section/topic/ENTERTAINMENT

We set up a code to pull from the RSS feeds every hour and append the new data to our datafile to build our final data file. We included duplicate articles because the content in the article itself may be changing or updated by the hour. The data is organized by what RSS feed it originated from, so that tech news and health news are stored seperately.

###Google News

![alt text](https://docs.google.com/uc?export=download&id=17XKXD6AMnENOX3fUB2iGq934POccI2af)

#Our Code

###Cron Code

07 \* \* \* \* /Users/macmain/anaconda3/bin/python3 /Users/macmain/2018FallTeamProject/get_hourly_info.py >> /Users/macmain/2018FallTeamProject/hourlyInfo.log 2>&1

20 23 \* \* \* /Users/macmain/anaconda3/bin/python3 /Users/macmain/2018FallTeamProject/CreateDailyInfo.py >> /Users/macmain/2018FallTeamProject/DailyInfo.log 2>&1

###Errors

Not all websites, despite indexing on the Google RSS feeds, allowed us to access their websites:
![alt text](https://docs.google.com/uc?export=download&id=1M-LjwYD1DXu5pfGyyTf1eV4DuGU-KP4M)

There was an error in the code for the first day of running the file that caused the headlines.txt file to grow exponentially with each running, resulting in an extremely large file after only a few hours. This was fixed in the code you will see below.
![alt text](https://docs.google.com/uc?export=download&id=1GhYt0pWiEJd1FSw65Kb0ONCFKGfxEZ0M)

The cron (and running the code in spyder) raised an error that we did not see in colab or jupyter, that there was a concern with a failure to properly idenitify the timezone of the timestamps.
![alt text](https://docs.google.com/uc?export=download&id=13B__aSYXKTOyuu1mgJpDRkGn89iSNf8P)

Cron also raised unicode errors that we did not have when running in jupyter and colab.
![alt text](https://docs.google.com/uc?export=download&id=1X4ZzmPnkL1DZSBc0dmKEhSKBFjQiwE4M)

###News Article Scraping
"""

!pip install feedparser #These are needed to run this in google colab
!pip install newspaper3k

import feedparser as fp
import json
import newspaper
import nltk
import re
import calendar
import os
import collections as co
from newspaper import Article
from time import mktime
from datetime import datetime
from pprint import pprint
from dateutil.parser import parse
from collections import Counter

nltk.download('punkt') #needed for the .nlp() call of the newspaper code

# Time Stamp for log file
print("")
print("")
print("")
print("========================================")
print("Log in time:", datetime.now())
print("========================================")
print("")
print("")
print("")

newspaper_json = json.dump({
  "google headlines": {
    "rss": "https://news.google.com/news/rss/"},
  "google world": {
     "rss": "https://news.google.com/news/rss/headlines/section/topic/WORLD"},
  "google business": {
     "rss": "https://news.google.com/news/rss/headlines/section/topic/BUSINESS"},
  "google nation": {
     "rss": "https://news.google.com/news/rss/headlines/section/topic/NATION"},
  "google health": {
     "rss": "https://news.google.com/news/rss/headlines/section/topic/HEALTH"},
  "google sports": {
     "rss": "https://news.google.com/news/rss/headlines/section/topic/SPORTS"},
  "google tech": {
     "rss": "https://news.google.com/news/rss/headlines/section/topic/TECHNOLOGY"},
  "google entertainment": {
     "rss": "https://news.google.com/news/rss/headlines/section/topic/ENTERTAINMENT"},
  }, open("grss.json","w"))

with open('grss.json',"r") as data_file:
    google = json.load(data_file)

data = {}
data['feeds'] = {}
datenow = datetime.now()
headlines_loc = './headlines.txt'

# Check to see if we made this file 
if os.path.exists(headlines_loc):
  with open('headlines.txt', 'r') as infile:
    ####headlines = json.load(infile) 
    headlines = infile.readlines() # Added 12/6/2018
    headline = [] #added 12/8/2018
else:
  headlines = []
  headline = [] #added 12/8/2018

"""This is the main body of the webscraping. It utilizes Feedparser package to access and iterate over the RSS feeds, and Newspaper to scrape the news articles. Newspaper is designed for news article scraping, and is a powerful tool that allows us to access the different aspects of an online news article across multiple news sites without having to individually identify where those items are located. 

In this case, it is fetching Authors, Publication Date, Headline, and the main text of the news article. It also has NLP functionality through NLTK. By calling .nlp(), it will pass the text of the article through NLTK and identify a list of keywords and five key sentences to put in the summary by a complicated metric (full code of the .nlp() function [here](https://github.com/codelucas/newspaper/blob/master/newspaper/nlp.py)).
"""

count = 1
for web, value in google.items():
    if 'rss' in value:
        d = fp.parse(value['rss'])
        file = {
            "rss": value['rss'],
            "articles": []}
        for entry in d.entries:
            if hasattr(entry, 'published'):
                article = {}
                article['link'] = entry.link
                date = entry.published_parsed
                article['published'] = datetime.fromtimestamp(mktime(date)).isoformat()
                try:
                    content = Article(entry.link)
                    content.download()
                    if content.title == [x for x in headlines]:
                      print("Already captured")
                      pass
                    else:
                      content.parse()
                      content.nlp()
                except Exception as e:
                    print(e)
                    continue
                article['title'] = content.title
                headline.append(content.title) #changed from headlines.append to headline.append 12/8/2018
                article['author'] = content.authors
                article['text'] = content.text
                article['keywords'] = content.keywords
                article['summary'] = content.summary
                #article['images'] = content.images #images generates a set, which cannot be saved in a dict
                file['articles'].append(article)
                print(count," RSS ",web, " url: ", entry.link)
                count = count + 1
            data['feeds'][web] = file

try:
    with open('article.json', 'w') as outfile:
        json.dump(data, outfile)
except Exception as e:
    print(e)
try:
    with open('headlines.txt', 'a+') as outfile:
      json.dump(headline, outfile) #changed from headlines to headline 12/8/2018 to fix issue with exponentially growing file. 
except Exception as e:
    print(e)

news = './article.json'
new = './articles.json'
# Check to see if we made this file 
if os.path.exists(new):
  with open(new) as infile:
    news_news = json.load(infile)
  news_new_dict = co.defaultdict(list)
  for f, a in news_news.items():
    #print(a)
    news_new_dict[f].append(a)
    news_new_dict[f]=news_new_dict[f][0]
    #pprint(news_new_dict[f][0]))
    
  #pprint(news_new_dict[f][1]) -- Commented out to handle cron's Unicode error
  with open(news) as infile:
    new_news = json.load(infile)
    for f, a in new_news.items():
      #print(a.keys())
      for b, c in a.items():
        for d in c['articles']:
          news_new_dict[b].append(d)
        #pprint(len(news_new_dict[b]))
    #pprint(news_new_dict[b][1]) -- Commented out to handle cron's Unicode error
  with open(new, "w") as outfile:
    json.dump(news_new_dict, outfile)
else:
  with open(news) as infile:
    new_news = json.load(infile)
    news_new_dict = co.defaultdict(list)
    for f, a in new_news.items():
      for b, c in a.items():
        for d in c['articles']:
          #print(b, d)
          news_new_dict[b].append(d)
    with open(new, "w") as outfile:
      json.dump(news_new_dict, outfile)

"""###Get Keywords by Hour

The following cell contains a function that returns a dictionary where the keys are types of news (entertainment, health, etc.) and the values are counter objects containing the number of times a keyword has appeared in an article of that news type.
"""

# Keywords per news type at this hour
def get_keywords_by_news_type():
    news_types = {}
    
    for google_news_type, news in data['feeds'].items():
        keywords = []
        # Ex: 'google entertainment' -> 'entertainment'
        news_type = re.split('google', google_news_type)[-1].strip()
        articles = news['articles']
        for article in articles:
            # Adds each article's list of keywords to a comprehensive list
            keywords.extend(article['keywords'])
            
        common_words = Counter(keywords)

        news_types[news_type] = common_words
    return news_types

keywords_by_type = get_keywords_by_news_type()
#pprint(keywords_by_type['health']) -- Commented out to handle cron's Unicode error

"""The following cell contains a function that returns a counter object of all keywords regardless of news type."""

# Key words regardless of news type at this hour
def get_keywords():
    keywords = []
    for google_news_type, news in data['feeds'].items():
        articles = news['articles']
        for article in articles:
            keywords.extend(article['keywords'])

    common_keywords = Counter(keywords) 
    return common_keywords

hourly_keywords = get_keywords()
#pprint(hourly_keywords) -- Commented out to handle cron's Unicode error

"""`get_date_id()` returns the date in DDMON notation (11DEC)"""

def get_date_id():
  date = datetime.now().date()
  today = datetime.today() ## Added 12/2.2018.
  month = calendar.month_abbr[today.month].upper()
  day = date.day

  date_id = str(day) + str(month)
  return date_id

date_id = get_date_id()

"""The following cell gets the current hour's `key_words_by_type` and appends it to a JSON file. If the JSON file hasn't been created yet, it creates it and writes the `key_words_by_type` dictionary to that json file. If the file already exists, it reads in the dictionary contained within the file, loops over the different news types, and adds the counter object attached to the news type from this hour to the counter object in the JSON file."""

f1_name = f'./{date_id}_hourly_keywords_by_type.json'
print(f1_name)

# Check to see if we made this file yet today
if os.path.exists(f1_name):
    # If we have, open file and load into by_type (dict)
    with open(f1_name) as infile:
        by_type = json.load(infile) # dict of news types to dict (which will be converted to counter)
        
    # Grab all news types that are present in last data set and current data set from this hourly call
    common_types = keywords_by_type.keys() & by_type.keys()
    # See if there are any new types of news in this hours call
    new_types = keywords_by_type.keys() - by_type.keys()
    
    new_dict = {}
    # Loop over all news types that exist in last dataset and current dataset
    for type_ in common_types:
        # Add the counter objects associated with each news type
        new_count = keywords_by_type[type_] + Counter(by_type[type_])
        # Create updated dict
        new_dict[type_] = new_count
    
    if new_types:
        # If there are new news types, insert them into the dict
        for type_ in new_types:
            new_dict[type_] = keywords_by_type[type_]  
    
    # "Overwrite" files data with new data, which contains the old data and this hour's data.
    with open(f1_name, 'w') as outfile:
        json.dump(new_dict, outfile)
else:
    # If file doesn't exist (first call at 00:00) create it.
    with open(f1_name, 'w') as outfile:
        json.dump(keywords_by_type, outfile)

"""The following cell gets the current hour's `hourly_keywords` and appends it to a JSON file. If the JSON file hasn't been created yet, it creates it and writes the `hourly_keywords` counter object to that json file. If the file already exists, it reads in the counter object contained within the file and adds the `hourly_keywords` counter object from this hour. It then writes the updated data back out to the json file."""

f2_name = f'./{date_id}_hourly_keywords.json'
print(f2_name)

# Check to see if we made this file yet today
if os.path.exists(f2_name):
    # If we have, open file and load into by_type (dict)
    with open(f2_name) as infile:
        # dict object that represents a counter
        all_keywords = json.load(infile)
    # Create counter object of all the old key words
    all_keywords_count = Counter(all_keywords)
    
    update_kw_count = all_keywords_count + hourly_keywords
    
    # "Overwrite" files data with new data, which contains the old data and this hour's data.
    with open(f2_name, 'w') as outfile:
        json.dump(update_kw_count, outfile)
else:
    # If file doesn't exist (first call at 00:00) create it.
    with open(f2_name, 'w') as outfile:
        json.dump(hourly_keywords, outfile)

"""###Get Keywords by Day"""

from datetime import datetime as dt

# Time Stamp for log file
print("")
print("")
print("")
print("========================================")
print("Log in time:", dt.now())
print("========================================")
print("")
print("")
print("")

date_id = get_date_id()

"""`get_daily_kw_by_type()` creates a file once at the end of each day that creates a dictionary of news types to their 30 most common keywords"""

def get_daily_kw_by_type():
    kw_by_type_f = f'./{date_id}_hourly_keywords_by_type.json'

    with open(kw_by_type_f) as infile:
        daily_keywords_by_type = json.load(infile)
        
    todays_kw_by_type = {}
    for type_, count in daily_keywords_by_type.items():
        most_common_by_type = Counter(count).most_common(30)
        todays_kw_by_type[type_] = most_common_by_type
    
    return todays_kw_by_type

"""`get_daily_kw_info()` returns the 50 most common keywords of the day regardless of news type."""

def get_daily_kw_info():
    kw_f = f'./{date_id}_hourly_keywords.json'

    with open(kw_f) as infile:
        daily_keywords = json.load(infile)
        
    daily_counts = Counter(daily_keywords).most_common(50)
    return daily_counts

"""Write `daily_kw_counts` and `kw_by_type` to their respective daily JSON files."""

daily_kw_counts = get_daily_kw_info()
kw_by_type = get_daily_kw_by_type()

daily_kw = './' +file_id + '_daily_Keywords_Info.json'
with open(daily_kw, 'w') as outfile:
    json.dump(daily_kw_counts, outfile)

daily_kw_by_type = './' +file_id + '_daily_Keywords_By_Type_Info.json'
with open(daily_kw_by_type, 'w') as outfile:
    json.dump(kw_by_type, outfile)

"""#Our Data

###Output

We have 4 main outputs from this code:

1. A dictionary of scraped news articles
2. A list of the headlines of all news articles in dictionary
3. A file of word counts per day
4. A file of word counts nested within RSS feed per day

In total, we have 16 files for a total of 164 MB of data:

1. articles.json (151 MB)

2. headlines.txt (1.97 MB)

3 and 4. (between 1-300 KB)

8DEC_daily__Keywords_By_Type_Info.json
    
8DEC_daily__Keywords_Info.json
   
8DEC_hourly_keywords.json
 
8DEC_hourly_keywords_by_type.json

9DEC_daily__Keywords_By_Type_Info.json

9DEC_daily__Keywords_Info.json

9DEC_hourly_keywords.json
 
9DEC_hourly_keywords_by_type.json

10DEC_daily__Keywords_By_Type_Info.json

10DEC_daily__Keywords_Info.json

10DEC_hourly_keywords.json

10DEC_hourly_keywords_by_type.json

11DEC_hourly_keywords.json

11DEC_hourly_keywords_by_type.json

###Structure

Articles.json has a structure like:

```
{"google headlines": [{
                       "link": Link to Article 
                       "published": YYYY-MM-DD HH:MM:SS
                       "title": Headline of News Article
                       "author": [Author Names]
                       "text": Full Text of Article
                       "keywords": [Words determined by Newspaper using NLTK to be important to the Article]
                       "summary": An extract of 2 to 3 sentences made by Newspaper using NLTK to determine informative sentences from the Article
                       }]

```

###Initial Analyses

After running it for three days, we were able to collect the following number of news articles (including repeat news stories):

<br>
  <table align = left><tr><td align = center><font size = 4>RSS</font></td><td align = center><font size =4><i>n</i></font></td></tr> 
  <tr><td><font size = 3> Google Headlines</font></td>, <td><font size = 3>2,554</font></td></tr>
  <tr><td><font size = 3>Google Business</font></td><td><font size = 3>4,938</font></td></tr>
  <tr><td><font size = 3>Google Nation</font></td><td><font size = 3>4,949</font></td></tr>
  <tr><td><font size = 3>Google World</font></td><td><font size = 3>4,946</font></td></tr>
  <tr><td><font size = 3>Google Health</font></td><td><font size = 3>4,356</font></td></tr>
  <tr><td><font size = 3>Google Sports</font></td><td><font size = 3>4,953</font></td></tr>
  <tr><td><font size = 3>Google Tech</font></td><td><font size = 3>4,957</font></td></tr>
  <tr><td><font size = 3>Google Entertainment</font></td><td><font size = 3>4,926</font></td></tr>
  <tr><td><font size = 4>Total</font></td><td><font size = 4>36,579</font></td></tr></table>

An example news story (from 'google entertainment' feed):
```
{'author': ['Evan Real', 'Thrnews Thr.Com'],
 'keywords': ['media',
              'song',
              'scores',
              'panther',
              'album',
              '2000',
              'best',
              'visual',
              'black',
              'nominated',
              'soundtrack',
              'grammys',
              'performance',
              'lamar',
              'nod'],
 'link': 'https://www.hollywoodreporter.com/news/grammy-nominations-2019-black-panther-star-is-born-shallow-movie-tv-nominees-1167406',
 'published': '2018-12-07T15:24:09',
 'summary': 'The nominations for the 2019 Grammys were unveiled on Friday '
            'morning, during which Black Panther became the third music '
            'soundtrack to earn an album of the year nod in the last 25 '
            'years.\n'
            "Black Panther — along with Lamar, who leads this year's "
            'nominations — made out with a total of five noms.\n'
            'Lamar also is nominated for "King\'s Dead," for best rap '
            'performance and best rap song, alongside Jay Rock, Future and '
            'James Blake.\n'
            'In the best score soundtrack for visual media, Black Panther is '
            'up against Blade Runner 2049, Coco, The Shape of Water and Star '
            'Wars: The Last Jedi.\n'
            'The chart-topping track is up for song of the year, record of the '
            'year, best pop duo/group performance and best song written for '
            'visual media.',
 'text': 'Along with soundtracks and songs from movies, notable Hollywood '
         "figures were nominated for music's highest honor, including Lady "
         'Gaga, Bradley Cooper and Donald Glover, among others.\n'
         '\n'
         'The nominations for the 2019 Grammys were unveiled on Friday '
         'morning, during which Black Panther became the third music '
         'soundtrack to earn an album of the year nod in the last 25 years. '
         'The LP features efforts from Kendrick Lamar, by whom the album was '
         'curated.\n'
         '\n'
         'Only two music soundtracks have received nominations in the last 25 '
         "years, including 1995's Waiting to Exhale and 2000's O Brother, "
         'Where Are Thou? Black Panther — along with Lamar, who leads this '
         "year's nominations — made out with a total of five noms. In addition "
         'to album of the year, Black Panther\'s pop-rap anthem "All the '
         'Stars," performed by Lamar and SZA, is up for record of the year, '
         'song of the year, best rap/sung performance and best song written '
         'for visual media. Lamar also is nominated for "King\'s Dead," for '
         'best rap performance and best rap song, alongside Jay Rock, Future '
         'and James Blake.\n'
         '\n'
         'Black Panther stands alongside fellow album of the year nominees '
         'Janelle Monae (Dirty Computer), Cardi B (Invasion of Privacy), '
         'Brandi Carlile (By the Way, I Forgive You), Drake (Scorpion), H.E.R. '
         '(H.E.R.), Post Malone (Beerbongs & Bentleys) and Kacey Musgraves '
         '(Golden Hour).\n'
         '\n'
         'In the best score soundtrack for visual media, Black Panther is up '
         'against Blade Runner 2049, Coco, The Shape of Water and Star Wars: '
         'The Last Jedi.\n'
         '\n'
         'While the soundtrack for A Star Is Born was not in the running at '
         'the Grammys this year because it was released on Oct. 5, its lead '
         'track "Shallow" by Lady Gaga and Bradley Cooper, was acknowledged by '
         'the Recording Academy. The chart-topping track is up for song of the '
         'year, record of the year, best pop duo/group performance and best '
         "song written for visual media. This marks Cooper's first time being "
         'nominated, while Gaga has been nominated 24 times in total. She has '
         'won six Grammys throughout her career.\n'
         '\n'
         'In the best song written for visual media category, "Shallow" and '
         '"All the Stars" face off against "Mystery of Love" (Call Me By Your '
         'Name), "Remember Me" (Coco) and "This Is Me" (The Greatest '
         'Showman).\n'
         '\n'
         'Other films and TV shows were honored in the best compilation '
         'soundtrack for visual media category, including Call Me By Your '
         'Name, Lady Bird, Deadpool 2, The Greatest Showman and Stranger '
         'Things.\n'
         '\n'
         'Aside from Lamar, Gaga and Cooper, other notable figures in '
         'Hollywood received noms. Donald Glover, aka Childish Gambino, was '
         'nominated in five categories. While "Feels Like Summer" was '
         'nominated for best R&B song, his politically charged single "This Is '
         'America" received noms for record of the year, song of the year, '
         'best rap/sung performance and best music video.\n'
         '\n'
         'Tiffany Haddish was nominated in the best spoken world album '
         'category for the audiobook reading of her 2017 memoir, The Last '
         'Black Unicorn. Dave Chappelle (Equanimity & the Bird Revelation), '
         'Patton Oswalt (Annihilation), Chris Rock (Tamborine), Fred Armisen '
         '(Standup For Drummers) and Jim Gaffigan (Noble Ape) are all up for '
         'best comedy album.\n'
         '\n'
         'And a roster of popular Broadway shows were honored with noms for '
         "best musical theater album, including The Band's Visit, Carousel, My "
         "Fair Lady and Once on This Island. NBC's Jesus Christ Superstar Live "
         'in Concert was also included.\n'
         '\n'
         'The 61st annual Grammy Awards airs on CBS on Feb. 10 at 8 p.m. ET.',
 'title': "Grammys: 'Black Panther' Scores First Album of the Year Soundtrack "
          'Nod Since 2000'}
          ```

For the top 50 higher frequency words for December 8th: 

```
[['game', 1034],
 ['trump', 888],
 ['president', 705],
 ['week', 545],
 ['video', 520],
 ['season', 468],
 ['health', 445],
 ['state', 412],
 ['team', 410],
 ['told', 406],
 ['right', 405],
 ['world', 382],
 ['study', 360],
 ['2018', 339],
 ['york', 335],
 ['games', 333],
 ['say', 331],
 ['white', 324],
 ['players', 321],
 ['company', 314],
 ['woman', 311],
 ['report', 301],
 ['deal', 281],
 ['best', 275],
 ['million', 274],
 ['thats', 257],
 ['trumps', 251],
 ['watch', 251],
 ['trailer', 247],
 ['way', 245],
 ['know', 245],
 ['win', 240],
 ['available', 236],
 ['court', 232],
 ['man', 227],
 ['hes', 227],
 ['times', 224],
 ['risk', 218],
 ['think', 216],
 ['feature', 216],
 ['work', 211],
 ['need', 209],
 ['apple', 209],
 ['market', 208],
 ['play', 206],
 ['washington', 205],
 ['try', 204],
 ['house', 203],
 ['things', 203],
 ['going', 199]]
```

###Possible Analyses

1. Sentiment Analysis of article headlines and text
2. Machine Learning to predict the news source based on n-gram usage in a given article
3. Analysis of word frequencies, for instance, across different news clusters
4. Burstiness (i.e. an increase in frequency over a finite time period) of particular news topics

**References**

Carlson, M. (2018). Automating judgment? Algorithmic judgment, news knowledge, and journalistic professionalism. New Media & Society, 20(5), 1755-1772.

 Galtung, J., & Ruge, M. H. (1965). The structure of foreign news: The presentation of the Congo, Cuba and Cyprus crises in four Norwegian newspapers. Journal of peace research, 2(1), 64-90.
 
Hamborg, F., Meuschke, N., & Gipp, B. (2018). Bias-aware news analysis using matrix-based news aggregation. International Journal on Digital Libraries, 1-19.
 
 Harcup, T., & O'Neill, D. (2001). What is news? Galtung and Ruge revisited. Journalism studies, 2(2), 261-280.
 
 O’Neill, D., & Harcup, T. (2009). News values and selectivity. In The handbook of journalism studies (pp. 181-194). Routledge.
# ---
"""